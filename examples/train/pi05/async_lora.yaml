# VLASH Training Configuration for PI05 with LoRA
# This config enables PEFT-based LoRA on top of the PI05 policy.

# Policy configuration
policy:
  type: pi05
  pretrained_path: lerobot/pi05_base
  push_to_hub: false
  dtype: bfloat16
  device: cuda
  state_cond: true

# Dataset configuration
dataset:
  repo_id: 
  video_backend: torchcodec

# Training parameters
output_dir: outputs/train/pi05_async_lora
job_name: pi05_async_lora
batch_size: 1
grad_accum_steps: 8  # Effective batch size per optimizer step: 1 x 8 = 8
steps: 50000
num_workers: 4
seed: 1000

# Optimizer configuration
use_policy_training_preset: false
optimizer:
  type: adamw
  lr: 5.0e-5
  betas: [0.9, 0.95]
  weight_decay: 1.0e-10

# Learning rate scheduler
scheduler:
  type: cosine_decay_with_warmup
  num_warmup_steps: 1000
  peak_lr: 5.0e-5
  decay_lr: 2.5e-6
  num_decay_steps: 50000

# Checkpointing
save_checkpoint: true
save_freq: 10000
eval_freq: 10000

# Logging
log_freq: 200
wandb:
  enable: true
  project: vlash
  entity: null
  disable_artifact: true

# VLASH-specific: Maximum temporal delay steps (set to 0 for standard training)
max_delay_steps: 8

# LoRA configuration
# The configuration is carefully crafted to maintain stability and performance.
lora:
  enable: true
  backend: peft
  extra_trainable_modules:  
    - action_in_proj
    - action_out_proj
    - time_mlp_in
    - time_mlp_out
    - state_proj
    - state_mlp_in
    - state_mlp_out
    - embeddings

    
  r: 16
  alpha: 16
  dropout: 0
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
    - out_proj
    - fc1
    - fc2
    - dense
    - embed_tokens
    - lm_head


